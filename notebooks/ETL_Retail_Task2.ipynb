{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dddeff99",
   "metadata": {},
   "source": [
    "# ETL: Online Retail → retail_dw.db (Section 1 Task 2)\n",
    "\n",
    "This notebook runs the ETL pipeline:\n",
    "- Extract: read data/data file\n",
    "- Transform: cleaning, TotalSales, categories, time dim\n",
    "- Load: write CustomerDim, ProductDim, TimeDim, SalesFact into outputs/db/retail_dw.db\n",
    "\n",
    "All outputs are saved under the `outputs/` folder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470c7705",
   "metadata": {},
   "source": [
    "## Setup and paths\n",
    "\n",
    "Import libraries and set up file paths and output directories. This cell defines where the input data should be and where outputs will be written."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f230849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA_PATH = c:\\Users\\HP\\Documents\\DSA2040_Practical_Exam_Geoffrey_Mwangi_566\\data\\Online_Retail.xlsx\n",
      "DB_PATH = c:\\Users\\HP\\Documents\\DSA2040_Practical_Exam_Geoffrey_Mwangi_566\\outputs\\db\\retail_dw.db\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "PROJECT_ROOT = Path.cwd().parent\n",
    "DATA_PATH = PROJECT_ROOT / 'data' / 'Online_Retail.xlsx'\n",
    "DB_PATH = PROJECT_ROOT / 'outputs' / 'db' / 'retail_dw.db'\n",
    "IMAGES_DIR = PROJECT_ROOT / 'outputs' / 'images'\n",
    "REPORTS_DIR = PROJECT_ROOT / 'outputs' / 'reports'\n",
    "(DB_PATH.parent).mkdir(parents=True, exist_ok=True)\n",
    "IMAGES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "REPORTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "print('DATA_PATH =', DATA_PATH)\n",
    "print('DB_PATH =', DB_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1334bab9",
   "metadata": {},
   "source": [
    "## Extract: read the source file\n",
    "\n",
    "Define a function to read the Excel file into a pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b3a66bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_retail(path=DATA_PATH):\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"Data file not found: {path}\")\n",
    "    if path.suffix in ('.xls','.xlsx'):\n",
    "        df = pd.read_excel(path, engine='openpyxl')\n",
    "    else:\n",
    "        df = pd.read_csv(path)\n",
    "    logging.info(f\"Read {len(df)} rows from {path.name}\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1adb0f9f",
   "metadata": {},
   "source": [
    "## Transform: cleaning and feature engineering\n",
    "\n",
    "This function:\n",
    "- parses `InvoiceDate` to datetime,\n",
    "- drops rows missing `InvoiceDate` or `CustomerID`,\n",
    "- converts `Quantity` and `UnitPrice` to numeric and removes invalid values,\n",
    "- computes `TotalSales = Quantity * UnitPrice`,\n",
    "- maps `Description` to a simple `Category`,\n",
    "- computes `invoice_date` and `is_weekend`,\n",
    "- filters to the last-year window (2024-08-12 → 2025-08-12) and falls back to the full cleaned dataset if empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c5573db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_transform(df):\n",
    "    df = df.copy()\n",
    "    # parse date\n",
    "    df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'], errors='coerce')\n",
    "    \n",
    "    # drop missing InvoiceDate or CustomerID\n",
    "    df = df.dropna(subset=['InvoiceDate','CustomerID'])\n",
    "    \n",
    "    # numeric conversions\n",
    "    df['Quantity'] = pd.to_numeric(df['Quantity'], errors='coerce').fillna(0).astype(int)\n",
    "    df['UnitPrice'] = pd.to_numeric(df['UnitPrice'], errors='coerce').fillna(0.0)\n",
    "    before = len(df)\n",
    "    df = df[(df['Quantity'] >= 0) & (df['UnitPrice'] > 0)]\n",
    "    logging.info(f'Removed {before - len(df)} rows with invalid Quantity/UnitPrice')\n",
    "    \n",
    "    # compute TotalSales\n",
    "    df['TotalSales'] = df['Quantity'] * df['UnitPrice']\n",
    "    \n",
    "    # category mapping\n",
    "    def categorize(desc):\n",
    "        if not isinstance(desc, str):\n",
    "            return 'Other'\n",
    "        d = desc.lower()\n",
    "        if any(k in d for k in ['usb','battery','camera','phone','charger','electronic','speaker']):\n",
    "            return 'Electronics'\n",
    "        if any(k in d for k in ['t-shirt','shirt','dress','jacket','coat','sweater']):\n",
    "            return 'Clothing'\n",
    "        if 'book' in d:\n",
    "            return 'Books'\n",
    "        if any(k in d for k in ['cup','mug','bottle','glass']):\n",
    "            return 'Home'\n",
    "        return 'Other'\n",
    "    df['Category'] = df['Description'].apply(categorize)\n",
    "    \n",
    "    # invoice_date and weekend flag\n",
    "    df['invoice_date'] = df['InvoiceDate'].dt.date\n",
    "    df['is_weekend'] = df['InvoiceDate'].dt.dayofweek.isin([5,6]).astype(int)\n",
    "    \n",
    "    # filter last year: 2024-08-12 -> 2025-08-12\n",
    "    cutoff_end = pd.to_datetime('2025-08-12')\n",
    "    cutoff_start = cutoff_end - pd.DateOffset(years=1)\n",
    "    df_last_year = df[(df['InvoiceDate'] >= cutoff_start) & (df['InvoiceDate'] <= cutoff_end)].copy()\n",
    "    if df_last_year.empty:\n",
    "        logging.warning('No rows in last-year window (2024-08-12 to 2025-08-12). Falling back to full cleaned dataset for ETL demonstration.')\n",
    "        df_last_year = df.copy()\n",
    "    else:\n",
    "        logging.info(f'Filtered to last year: {len(df_last_year)} rows between {cutoff_start.date()} and {cutoff_end.date()}')\n",
    "    return df_last_year\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eef00e4",
   "metadata": {},
   "source": [
    "## Build dimensions and SalesFact\n",
    "\n",
    "This function builds:\n",
    "- CustomerDim (aggregations per customer),\n",
    "- ProductDim (unique products with product_id),\n",
    "- TimeDim (per invoice date),\n",
    "- SalesFact (fact rows referencing dims)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed88a274",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dims_and_fact(df):\n",
    "    # CustomerDim\n",
    "    cust = df.groupby('CustomerID').agg(\n",
    "        total_purchases = ('TotalSales','sum'),\n",
    "        num_invoices = ('InvoiceNo', lambda x: x.nunique()),\n",
    "        country = ('Country', lambda x: x.mode().iloc[0] if len(x)>0 else None)\n",
    "    ).reset_index().rename(columns={'CustomerID':'customer_id'})\n",
    "    cust['customer_id'] = cust['customer_id'].astype(int)\n",
    "    cust = cust[['customer_id','country','total_purchases','num_invoices']]\n",
    "\n",
    "    # ProductDim\n",
    "    prod = df[['StockCode','Description','Category']].drop_duplicates().rename(columns={'StockCode':'stock_code','Description':'description','Category':'category'}).reset_index(drop=True)\n",
    "    prod['product_id'] = prod.index + 1\n",
    "    prod = prod[['product_id','stock_code','description','category']]\n",
    "\n",
    "    # TimeDim\n",
    "    time_df = pd.DataFrame({'invoice_date': sorted(df['invoice_date'].unique())})\n",
    "    time_df['year'] = pd.to_datetime(time_df['invoice_date']).dt.year\n",
    "    time_df['month'] = pd.to_datetime(time_df['invoice_date']).dt.month\n",
    "    time_df['quarter'] = pd.to_datetime(time_df['invoice_date']).dt.quarter\n",
    "    time_df['time_id'] = time_df.index + 1\n",
    "    time_df = time_df[['time_id','invoice_date','year','month','quarter']]\n",
    "\n",
    "    # SalesFact\n",
    "    prod_map = {r['stock_code']: r['product_id'] for _,r in prod.iterrows()}\n",
    "    time_map = {r['invoice_date']: r['time_id'] for _,r in time_df.iterrows()}\n",
    "    fact = df.copy()\n",
    "    fact['product_id'] = fact['StockCode'].map(prod_map)\n",
    "    fact['customer_id'] = fact['CustomerID'].astype(int)\n",
    "    fact['time_id'] = fact['invoice_date'].map(time_map)\n",
    "    fact = fact[['InvoiceNo','product_id','customer_id','time_id','Quantity','UnitPrice','TotalSales']].rename(columns={'InvoiceNo':'invoice_no','Quantity':'quantity','UnitPrice':'unit_price','TotalSales':'total_sales'})\n",
    "    fact = fact.dropna(subset=['product_id','customer_id','time_id'])\n",
    "    return cust, prod, time_df, fact\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73d5c05",
   "metadata": {},
   "source": [
    "## Load: write DataFrames to SQLite\n",
    "\n",
    "Write the dims and fact tables into the SQLite DB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "310a7ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_to_sqlite(cust, prod, time_df, fact, db_path=DB_PATH):\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cust.to_sql('CustomerDim', conn, if_exists='replace', index=False)\n",
    "    prod.to_sql('ProductDim', conn, if_exists='replace', index=False)\n",
    "    time_df.to_sql('TimeDim', conn, if_exists='replace', index=False)\n",
    "    fact.to_sql('SalesFact', conn, if_exists='replace', index=False)\n",
    "    conn.commit()\n",
    "    # log counts\n",
    "    for t in ['CustomerDim','ProductDim','TimeDim','SalesFact']:\n",
    "        c = pd.read_sql_query(f'SELECT COUNT(*) as cnt FROM {t}', conn)\n",
    "        logging.info(f\"{t} rows: {c['cnt'].iat[0]}\")\n",
    "    conn.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8642cd",
   "metadata": {},
   "source": [
    "## Run the full ETL pipeline\n",
    "\n",
    "This final cell contains the commands to run the ETL end-to-end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f863d787",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-14 22:13:51,863 - INFO - Read 541909 rows from Online_Retail.xlsx\n",
      "2025-08-14 22:13:52,113 - INFO - Removed 8945 rows with invalid Quantity/UnitPrice\n",
      "2025-08-14 22:13:53,633 - WARNING - No rows in last-year window (2024-08-12 to 2025-08-12). Falling back to full cleaned dataset for ETL demonstration.\n",
      "2025-08-14 22:13:56,180 - INFO - CustomerDim rows: 4338\n",
      "2025-08-14 22:13:56,180 - INFO - ProductDim rows: 3897\n",
      "2025-08-14 22:13:56,180 - INFO - TimeDim rows: 305\n",
      "2025-08-14 22:13:56,199 - INFO - SalesFact rows: 397884\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ETL complete. DB at c:\\Users\\HP\\Documents\\DSA2040_Practical_Exam_Geoffrey_Mwangi_566\\outputs\\db\\retail_dw.db\n",
      "Ready: uncomment the ETL run cell to execute the pipeline.\n"
     ]
    }
   ],
   "source": [
    "df_raw = read_retail()\n",
    "df_clean = clean_transform(df_raw)\n",
    "cust, prod, time_df, fact = build_dims_and_fact(df_clean)\n",
    "load_to_sqlite(cust, prod, time_df, fact)\n",
    "print('ETL complete. DB at', DB_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
